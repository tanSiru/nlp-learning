{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Chapter 1: Finding words, phrases, names and concepts\"\"\"\n",
    "#importing the english language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#the nlp object/processor which is a pipeline\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mrs.\n",
      "Smith\n",
      "you\n",
      "look\n",
      "nice\n",
      ".\n",
      "How\n",
      "are\n",
      "you\n",
      "today\n",
      "?\n",
      "Smith Smith\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tokenzing in spacy\n",
    "when using spacy nlp objects to process texts it will create a doc object\n",
    "\"\"\"\n",
    "doc = nlp(\"Mrs. Smith you look nice. How are you today?\")\n",
    "doc1 = doc[1]\n",
    "#getting the token text via .text attribute\n",
    "doc1_text = doc1.text\n",
    "for i in doc:\n",
    "    print(i)\n",
    "print(doc1,doc1_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mrs.', 'Smith', 'you', 'look', 'nice', '.', 'How', 'are', 'you', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "txt = \"Mrs. Smith you look nice. How are you today?\"\n",
    "print(word_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mrs.', 'Smith', 'you', 'look', 'nice', '.', 'How', 'are', 'you', 'today', '?']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Span in spaCy\n",
    "span is a slice of a obejcts that consits of 1<=\n",
    "it's only a view of the object and doesn't provide any data\n",
    "\"\"\"\n",
    "#a span can be created via a python slice\n",
    "txt = doc[1:3]\n",
    "\n",
    "#we can directly get the index of token\n",
    "print([token.text for token in doc])\n",
    "print([token.i for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_alpha [False, True, True, True, True, False, True, True, True, True, False, True]\n",
      "is_punct: [False, False, False, False, False, True, False, False, False, False, True, False]\n",
      "like_num: [False, False, False, False, False, False, False, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Returns a boolean value:\n",
    "is_alpha: whther the char is an english alpahbet\n",
    "is_punct: whether it's punctutation\n",
    "like_num: is the similar to a number(10 or ten would be return True)\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(\"Mrs. Smith you look nice. How are you today? one\")\n",
    "\n",
    "print(\"is_alpha\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Statistical models\n",
    "it allows spacy to make predictions based on contexts\n",
    "ex: POS, Syntactic dependencies, Named Enities\n",
    "spaCy comes with a pre-trained model packages for use\n",
    "it can be downloaded via (python -m spacy download en_core_web_sm)\n",
    "\"\"\"\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") #trained on web texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRON\n",
      "VERB\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"She likes apple\")\n",
    "for token in doc:\n",
    "    #in spacy attributes that return string usually end in an underscore, otherwise it returns an int\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj likes\n",
      "likes VERB ROOT likes\n",
      "apple NOUN dobj likes\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A dependency parser analyzes the grammatical structure of a sentence, \n",
    "establishing relationships between \"head\" words and words which modify those heads.\n",
    "\"\"\"\n",
    "for token in doc:\n",
    "    #dep_: dependency\n",
    "    #token.head:the parent token the word is attached to \n",
    "    print(token.text, token.pos_,token.dep_,token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Twitter PRODUCT\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Named Entity\"\"\"\n",
    "\n",
    "#seems like captilization is important\n",
    "doc = nlp(\"Apple is looking at Twitter\")\n",
    "\n",
    "for ne in doc.ents:\n",
    "    print(ne.text, ne.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "determiner\n",
      "Objects, vehicles, foods, etc. (not services)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Getting help from spacy\"\"\"\n",
    "print(spacy.explain(\"dobj\"))\n",
    "print(spacy.explain(\"det\"))\n",
    "print(spacy.explain(\"PRODUCT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "\"\"\"spaCy Match\"\"\"\n",
    "\"\"\"\n",
    "instead of re, spacy Match on Doc objects, token and str,whereas re only works on str\n",
    "it can also search for other lexical attributes\n",
    "rules can be written for the model's prediction\n",
    "ex: find duck if only it is a verb\n",
    "Match patterns are list of dicts:\n",
    "token texts:[{\"TEXT\":\"iPhone\"},{\"TEXT\":\"X\"}]\n",
    "lexical attributes:[{\"LOWER\":\"iphone\"},{\"LOWER\":\"x\"}]\n",
    "any token attributes: [{\"LEMMA\":\"buy\",\"POS\":\"NOUN\"}]\n",
    "each dict contains the key-value pair of the names of token attribute and their expected values\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "#import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "#load the model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# #intialize the matcher with the shared vocab\n",
    "#will be explained, rn just remember to pass it in\n",
    "matcher = Matcher(nlp.vocab)\n",
    "#creating a pattern and adding it to Matcher\n",
    "pattern=[{\"TEXT\":\"iPhone\"},{\"TEXT\":\"X\"}]\n",
    "#1st arg:unique id to identify which pattern was matched\n",
    "#2nd arg: optional callback\n",
    "#3rd arg: the pattern\n",
    "#https://stackoverflow.com/questions/66164156/problem-with-using-spacy-matcher-matcher-matcher-add-method\n",
    "matcher.add(\"IPHONE_PATTRN\",[pattern])\n",
    "\n",
    "doc = nlp(\"upcoming iPhone X release date\")\n",
    "#this will return a list of tuples(match-id,start index,end index)\n",
    "matched=matcher(doc)\n",
    "\n",
    "for matchID,start,end in matched:\n",
    "    #matchID: hash id \n",
    "    print(doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLB The Show 2018\n"
     ]
    }
   ],
   "source": [
    "pattern = [{\"LOWER\":\"mlb\"},{\"LOWER\":\"the\"},{\"LOWER\":\"show\"},{\"IS_DIGIT\":True}]\n",
    "matcher.add(\"MLB\",[pattern])\n",
    "doc=nlp(\"MLB The Show 2018 > FIFA 2020\")\n",
    "matched = matcher(doc)\n",
    "for x,y,z in matched:\n",
    "    print(doc[y:z].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love pizza\n",
      "love ball\n"
     ]
    }
   ],
   "source": [
    "#when the love word has verb speech-tag followed by a noun\n",
    "pattern = [{\"LEMMA\":\"love\",\"POS\":\"VERB\"},{\"POS\":\"NOUN\"}]\n",
    "matcher.add(\"MLB\",[pattern])\n",
    "doc=nlp(\"i love pizza and i love ball\")\n",
    "matched=matcher(doc)\n",
    "for x,y,z in matched:\n",
    "    print(doc[y:z].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "MLB The Show 2018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOP can have these values:\\n!: match 0 times\\n?: 0 or 1 times\\n+: 1 or more times\\n*: 0 or more times\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"using identifiers and modifiers\"\"\"\n",
    "pattern = [{\"LEMMA\":\"buy\"},{\"POS\":\"DET\",\"OP\":\"+\"},{\"POS\":\"NOUN\"}]\n",
    "pattern2 = [{\"LOWER\":\"mlb\"},{\"LOWER\":\"the\"},{\"LOWER\":\"show\"},{\"IS_DIGIT\":True}]\n",
    "# pattern = [{\"LEMMA\":\"buy\"},{\"POS\":\"DET\",\"OP\":\"?\"},{\"POS\":\"NOUN\"}]\n",
    "#{\"POS\":\"DET\",\"OP\":\"?\"}: OP: operator, ?: a quantifier(similar to modifier for re) and in this 0 or 1 times\n",
    "matcher.add(\"MLB\",[pattern,pattern2])\n",
    "doc=nlp(\"I bought a smartphone. Now I'm buying apps. I also like MLB The Show 2018\")\n",
    "matched=matcher(doc)\n",
    "for x,y,z in matched:\n",
    "    print(doc[y:z].text)\n",
    "# for i in doc:\n",
    "#     print(i,i.pos_)\n",
    "\n",
    "\"\"\"\n",
    "OP can have these values:\n",
    "!: match 0 times\n",
    "?: 0 or 1 times\n",
    "+: 1 or more times\n",
    "*: 0 or more times\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401\n",
      "3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Chapter 2: Large-scale data analysis with spaCy\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Vocab: stores all shared data in vocab, it contains the word and its tags\n",
    "spaCy encodes all strs to hash codes\n",
    "str are only stored once in the StringStore via nlp.vocab.strings\n",
    "String Sotre:look up values in both directions(hash->str||str->hash)\n",
    "\"\"\"\n",
    "\n",
    "word_hash=nlp.vocab.strings[\"coffee\"]\n",
    "word_str=nlp.vocab.strings[word_hash]\n",
    "print(word_str,word_hash)\n",
    "\n",
    "#we can also use/do this via doc\n",
    "doc = nlp(\"coffee is overrated\")\n",
    "print(doc.vocab.strings[\"coffee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "#lexmes objects are an context-independent entry in vocab\n",
    "lexme = nlp.vocab[\"coffee\"]\n",
    "print(lexme.text,lexme.orth,lexme.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating a Doc manually\"\"\"\n",
    "\"\"\"\n",
    "although a Doc object is automaticlly created when creating a nlp\n",
    "a Doc object can also be created manually\n",
    "\"\"\"\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc, Span\n",
    "nlp = English()\n",
    "words = [\"Hello\",\"World\",\"!\"]\n",
    "spaces = [True,False,False]\n",
    "#spaces=spaces: whether the word will be followed by a space, this is also includes the word\n",
    "doc = Doc(nlp.vocab,words=words,spaces=spaces)\n",
    "\n",
    "#spans can also be created manually\n",
    "span = Span(doc,0,2)\n",
    "#labeling the span\n",
    "span_labeled=Span(doc ,0,2)\n",
    "#adding a ent to doc\n",
    "doc.ents = [span_labeled]\n",
    "for i in doc.ents:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420460095864367"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Word Vector and semantic similairty\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "comparing words and their similaities requires a larger model than \"en_core_web_sm\"(small)\n",
    "valid ones are: \"en_core_web_md\"(medium) & \"en_core_web_lg\"(large)\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "# download the large version later\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "#what is depended by? subject? meaning? target?\n",
    "\n",
    "\"DOC\"\n",
    "doc1 = nlp(\"I like chicken\")\n",
    "doc2 = nlp(\"I like beef\")\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.795485"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"TOKEN\"\n",
    "t1,t2=doc1[2],doc2[2]\n",
    "t1.similarity(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5900396443523004"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"DOC AND TOKEN\"\n",
    "t2 = doc2[2]\n",
    "doc1.similarity(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.608555753616397"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"SPAN AND DOC\"\n",
    "doc = nlp(\"I like fruits and potatoes\")\n",
    "span = doc[2:5]\n",
    "span.similarity(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great restaurant.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "print(doc[3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello intj\n",
      "how advmod\n",
      "are ROOT\n",
      "you nsubj\n"
     ]
    }
   ],
   "source": [
    "from ideas import p1,p2\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "doc1 = nlp(p1)\n",
    "doc2 = nlp(p2)\n",
    "\n",
    "doc1.similarity(doc2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mango juice\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Combining Rules and models\"\"\"\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import en_core_web_md as md\n",
    "#PhraseMatcher is faster than Matcher which is better for a large text size\n",
    "\n",
    "nlp = md.load()\n",
    "\n",
    "pattern = nlp(\"Mango Juice\")\n",
    "pattern2 = nlp(\"mango juice\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "#instead of taking list of Dicts as args it instead takes Doc objects\n",
    "matcher.add(\"MANGO\",[pattern,pattern2])\n",
    "\n",
    "text = \"I love apple juice and mango juice\"\n",
    "text=nlp(text)\n",
    "for matchId,start,end in matcher(text):\n",
    "    print(text[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Harry Potter,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Chpater 3: Pipelines\"\"\"\n",
    "\"\"\"\n",
    "Custom Pipeline:\n",
    "we can add our own function that will be executed when the nlp object is called\n",
    "\"\"\"\n",
    "from spacy.language import Language\n",
    "import spacy\n",
    "\n",
    "# @Language.component(\"custom_work\")\n",
    "def custom_work(doc):\n",
    "    print(\"worked\")\n",
    "    return doc\n",
    "\n",
    "#last=True: add its as the last \n",
    "#first=True: add its as the first\n",
    "# before: before a certain pipeline\n",
    "# after: after a certain pipeline \n",
    "# nlp.add_pipe(\"custom_work\")\n",
    "nlp.pipe_names\n",
    "txt = nlp(\"This is Harry Potter\")\n",
    "\n",
    "txt.ents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Custom Attributes\"\"\"\n",
    "\"\"\"\n",
    "Features: add custom metatdata, to docs, tokens, and spans\n",
    "used via the \"._\" property for custom attributes\n",
    "this makes it is to differenitate whether it was made by the user or not\n",
    "\"\"\"\n",
    "\n",
    "#to set attributes it needs to be set on global classes\n",
    "from spacy.tokens import Doc,Token,Span\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "try:\n",
    "    Doc.set_extension(\"title\",default=None)\n",
    "    Span.set_extension(\"has_color\",default=False)\n",
    "    Token.set_extension(\"is_color\",default=False)\n",
    "except:\n",
    "    print(0)\n",
    "\n",
    "doc = nlp(\"This is my doc. This is blue\")\n",
    "\n",
    "doc._.title = \"My doc\"\n",
    "for token in doc:\n",
    "    if token.text == \"blue\":\n",
    "        token._.is_color=True\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Types of extensions:\n",
    "Attribute, Property, Method\n",
    "\"\"\"\n",
    "\n",
    "doc._.title\n",
    "doc[7]._.is_color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Getter\"\"\"\n",
    "\n",
    "#Property extensions\n",
    "def get_rgb(token):\n",
    "    rgb = [\"red\",\"green\",\"blue\"]\n",
    "    return token.text.lower() in rgb\n",
    "\n",
    "try:\n",
    "    Token.set_extension(\"is_rgb\",getter=get_rgb)\n",
    "except:\n",
    "    pass\n",
    "#TRY A SETTER NEXT\n",
    "doc = nlp(\"This is blue. BLUE\")\n",
    "doc[2]._.is_rgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Span Extenstions should always use a getter\"\"\"\n",
    "#Property extensions\n",
    "def get_if_rgb(span):\n",
    "    rgb = [\"red\", \"green\", \"blue\"]\n",
    "    return any(token.text.lower() in rgb for token in span)\n",
    "try:\n",
    "    Span.set_extension(\"has_color\",getter=get_if_rgb,force=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "doc[0:4]._.has_color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Method extension\"\"\"\n",
    "\"\"\"\n",
    "Features:\n",
    "assign functions that can be called as an object method\n",
    "allows passing of args\n",
    "\"\"\"\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def has_token(doc,txt):\n",
    "    return txt.lower() in [token.text.lower() for token in doc]\n",
    "\n",
    "# Doc.set_extension(\"has_token\",method=has_token)\n",
    "\n",
    "doc=nlp(\"I like the color red.\")\n",
    "\n",
    "doc._.has_token(\"RED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "# Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)\n",
    "    # print(ent)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"exercises/en/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "\n",
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(countries_component)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"SLOW\"\\ndocs = [nlp(text) for texts in MANY_TEXTS]\\n\"FAST\"\\nnlp.pipe will make faster since it yields objects\\ndocs = list(nlp.pipe(MANY_TEXTS))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Processing and performance\"\"\"\n",
    "import spacy,en_core_web_md\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "\"\"\"\n",
    "\"SLOW\"\n",
    "docs = [nlp(text) for texts in MANY_TEXTS]\n",
    "\"FAST\"\n",
    "nlp.pipe will make faster since it is a generator and yields objects\n",
    "docs = list(nlp.pipe(MANY_TEXTS))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setting as_tuples=True on nlp.pipe lets you pass in (text, context) tuples\n",
    "Yields (doc, context) tuples\n",
    "Useful for associating metadata with the doc\n",
    "\"\"\"\n",
    "\n",
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"page_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extensions and pipe\"\"\"\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"id\", default=None)\n",
    "Doc.set_extension(\"page_number\", default=None)\n",
    "\n",
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\": 15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\": 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context[\"id\"]\n",
    "    doc._.page_number = context[\"page_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Tokenizing only\"\"\"\n",
    "\"slow\"\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "doc[0].like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"this returns a Doc object and only the words tokenized\"\n",
    "\"so this will be a lot faster\"\n",
    "doc = nlp.make_doc(\"The sky is blue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Disabling pipelines\"\"\"\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text=\"Joe Biden\"\n",
    "with nlp.disable_pipes(\"parser\",\"ner\"):\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie\n",
      "Angela Merkel\n",
      "Lady Gaga\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "patterns = [nlp(person) for person in people]\n",
    "\n",
    "for i in patterns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie\n",
      "Angela Merkel\n",
      "Lady Gaga\n"
     ]
    }
   ],
   "source": [
    "patterns = list(nlp.pipe(people))\n",
    "\n",
    "for i in patterns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Chapter 4\"\"\"\n",
    "\n",
    "\"\"\"Training Loops\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Steps of traning:\n",
    "1)Loop for a number of times.\n",
    "2)Shuffle the training data.\n",
    "3)Divide the data into batches.\n",
    "4)Update the model for each batch.\n",
    "5)Save the updated model.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" The\n",
    "numbers printed to the shell represent the loss on each iteration, the amount of\n",
    "work left for the optimizer. The lower the number, the better.\n",
    "\"\"\"\n",
    "\"Do both sentiment analysis using both spacy and nltk to see which gets better results\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
